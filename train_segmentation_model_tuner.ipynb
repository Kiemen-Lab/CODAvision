{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-23T16:35:57.030Z",
     "start_time": "2024-08-23T16:35:57.013495Z"
    }
   },
   "source": [
    "# https://keras.io/examples/vision/deeplabv3_plus/\n",
    "\"\"\"\n",
    "Author: Valentina Matos (Johns Hopkins - Wirtz/Kiemen Lab)\n",
    "Date: June 17, 2024\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_VLOG_LEVEL\"] = \"2\"\n",
    "os.system(\"nvcc --version\")\n",
    "from glob import glob\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import io as tf_io\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T16:35:57.434199Z",
     "start_time": "2024-08-23T16:35:57.417692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Inputs \n",
    "pth = r'\\\\10.99.68.52\\Kiemendata\\Valentina Matos\\tissues for methods paper\\human liver'\n",
    "nm = 'CODA_python_08_19_2024'\n",
    "pthDL = os.path.join(pth, nm)\n",
    "fine_tune=False"
   ],
   "id": "eaac3febfe38fcb7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T16:35:59.993747Z",
     "start_time": "2024-08-23T16:35:58.110135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure TensorFlow is set to use the GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Set memory growth to avoid using all GPU memory\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        logical_devices = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"TensorFlow is using the following GPU: {logical_devices[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available. Ensure that the NVIDIA GPU and CUDA are correctly installed.\")"
   ],
   "id": "7acea264c6c91af7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using the following GPU: LogicalDevice(name='/device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T16:11:44.383559Z",
     "start_time": "2024-08-23T16:11:44.380555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load net data\n",
    "with open(os.path.join(pthDL, 'net.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    classNames = data['classNames']\n",
    "    IMAGE_SIZE = data['sxy']\n",
    "    nm = data['nm']\n",
    "    if 'model' in f:\n",
    "        raise ValueError(f'A network has already been trained for model {nm}. Choose a new model name to retrain.')"
   ],
   "id": "24c42934a0a1202a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create dataset\n",
    "\n",
    "# Clear previous models data to optimize GPU performance\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define paths to training and validation directories\n",
    "pthTrain = os.path.join(pthDL, 'training')\n",
    "pthValidation = os.path.join(pthDL, 'validation')\n",
    "\n",
    "# Get paths to training images and labels\n",
    "train_images = sorted(glob(os.path.join(pthTrain, 'im', \"*.png\")))\n",
    "train_masks = sorted(glob(os.path.join(pthTrain, 'label', \"*.png\")))\n",
    "\n",
    "# Get paths to validation images and labels\n",
    "val_images = sorted(glob(os.path.join(pthValidation, 'im', \"*.png\")))\n",
    "val_masks = sorted(glob(os.path.join(pthValidation, 'label', \"*.png\")))\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 3\n",
    "NUM_CLASSES = len(classNames)  # Number of classes\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "def read_image(image_path, mask=False):\n",
    "    image = tf_io.read_file(image_path)\n",
    "    if mask:\n",
    "        image = tf_image.decode_png(image, channels=1)\n",
    "        image.set_shape([None, None, 1])\n",
    "        image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    else:\n",
    "        image = tf_image.decode_png(image, channels=3)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    return image\n",
    "\n",
    "def load_data(image_list, mask_list):\n",
    "    image = read_image(image_list)\n",
    "    mask = read_image(mask_list, mask=True)\n",
    "    return image, mask\n",
    "\n",
    "def data_generator(image_list, mask_list):\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((image_list, mask_list))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = data_generator(train_images, train_masks)\n",
    "val_dataset = data_generator(val_images, val_masks)"
   ],
   "id": "9635b9314fc1db3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#_____________________Build DeepLabV3+ model_____________________\n",
    "def convolution_block(\n",
    "        block_input,\n",
    "        num_filters=256,\n",
    "        kernel_size=3,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=\"same\",\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=keras.initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def DeeplabV3Plus(image_size, num_classes):\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    preprocessed = tf.keras.applications.resnet50.preprocess_input(model_input)\n",
    "    resnet50 = tf.keras.applications.ResNet50(\n",
    "        weights=\"imagenet\", include_top=False, input_tensor=preprocessed\n",
    "    )\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
    "    return keras.Model(inputs=model_input, outputs=model_output)\n",
    " \n",
    " \n",
    "def DeeplabV3PlusTuning(image_size, num_classes,hp):\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    preprocessed = tf.keras.applications.resnet50.preprocess_input(model_input)\n",
    "    resnet50 = tf.keras.applications.ResNet50(\n",
    "        weights=\"imagenet\", include_top=False, input_tensor=preprocessed\n",
    "    )\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    \n",
    "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
    "    \n",
    "    # Use hyperparameters to tune the learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    model = keras.Model(inputs=model_input, outputs=model_output)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n"
   ],
   "id": "2e7cd615d2ceee27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_model(hp):\n",
    "    return DeeplabV3PlusTuning(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES, hp=hp)"
   ],
   "id": "f34f5fc48f3ffd4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7fdb02c01c7b0124"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Custom callbacks for validations\n",
    "\n",
    "class BatchAccCall(keras.callbacks.Callback):\n",
    "    def __init__(self, val_data, num_validations=3, early_stopping = True, reduceLRonPlateau = True, monitor='val_accuracy', ES_patience=6, RLRoP_patience=3, factor=0.1, verbose=0,\n",
    "                 save_best_model=True, filepath='best_model.h5'):\n",
    "        super(BatchAccCall, self).__init__()\n",
    "        self.batch_accuracies = []\n",
    "        self.batch_numbers = []\n",
    "        self.batch_losses = []\n",
    "        self.epoch_indices = []\n",
    "        self.epoch_numbers = []\n",
    "        self.current_epoch = 0\n",
    "        self.val_data = val_data\n",
    "        self.num_validations = num_validations\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracies = []\n",
    "        self.val_indices = []\n",
    "        self.early_stopping = early_stopping\n",
    "        self.RLRoP = reduceLRonPlateau\n",
    "        self.monitor = monitor\n",
    "        self.ES_patience = ES_patience\n",
    "        self.RLRoP_patience = RLRoP_patience\n",
    "        self.original_RLRoP_patience = RLRoP_patience\n",
    "        self.verbose = verbose\n",
    "        if monitor not in ['val_accuracy', 'val_loss']:\n",
    "            raise ValueError(\"Monitor can only be 'val_loss' or 'val_accuracy'\")\n",
    "        self.mode = 'min' if monitor == 'val_loss' else 'max'\n",
    "        self.save_best_model = save_best_model\n",
    "        self.save_path = filepath\n",
    "        self.monitor_op = np.less if self.mode == 'min' else np.greater\n",
    "        self.best = np.Inf if self.mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.factor = factor\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch\n",
    "        self.epoch_indices.append(self.current_epoch)\n",
    "        self.validation_steps = np.linspace(0, self.params['steps'], self.num_validations + 1, dtype=int)[1:]\n",
    "        self.validation_counter = 0\n",
    "        self.current_step = 0\n",
    "\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_end = time.time()\n",
    "        logs = logs or {}\n",
    "        self.current_step += 1\n",
    "        if self.current_step in self.validation_steps:\n",
    "            self.run_validation()\n",
    "            self.val_indices.append(self.current_step+self.params['steps']*(self.current_epoch))\n",
    "        accuracy = logs.get('accuracy')  # Use the metric name you specified\n",
    "        if accuracy is not None:\n",
    "            self.batch_accuracies.append(accuracy)\n",
    "            self.batch_numbers.append(self.params['steps'] * self.current_epoch + batch + 1)\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            self.batch_losses.append(loss)\n",
    "\n",
    "    def run_validation(self):\n",
    "        val_loss_total = 0\n",
    "        val_accuracy_total = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x_val, y_val in self.val_data:\n",
    "            y_val = tf.cast(y_val, dtype=tf.int32)\n",
    "            val_logits = self.model(x_val, training=False)\n",
    "            num_classes = val_logits.shape[-1]\n",
    "            val_logits_flat = tf.reshape(val_logits, [-1, num_classes])\n",
    "            y_val_flat = tf.reshape(y_val, [-1])\n",
    "            predictions = tf.argmax(val_logits_flat, axis=1)\n",
    "            predictions = tf.cast(predictions, dtype=tf.int32)\n",
    "            val_loss = tf.keras.losses.sparse_categorical_crossentropy(y_val_flat, val_logits_flat, from_logits=True)\n",
    "            val_accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_val_flat), tf.float32))\n",
    "\n",
    "            val_loss_total += tf.reduce_mean(val_loss).numpy()\n",
    "            val_accuracy_total += val_accuracy.numpy()\n",
    "            num_batches += 1\n",
    "\n",
    "        val_loss_avg = val_loss_total / num_batches\n",
    "        val_accuracy_avg = val_accuracy_total / num_batches\n",
    "\n",
    "        self.validation_losses.append(val_loss_avg)\n",
    "        self.validation_accuracies.append(val_accuracy_avg)\n",
    "\n",
    "        #print(f'Learning rate:{float(tf.keras.backend.get_value(self.model.optimizer.lr))}')\n",
    "        #print(f\"Validation at step {self.current_step}: loss = {val_loss_avg}, accuracy = {val_accuracy_avg}\")\n",
    "\n",
    "        if self.early_stopping or self.RLRoP:\n",
    "            if self.monitor == 'val_loss':\n",
    "                current = val_loss_avg\n",
    "            elif self.monitor == 'val_accuracy':\n",
    "                current = val_accuracy_avg\n",
    "\n",
    "            if current is None:\n",
    "                return\n",
    "\n",
    "            if self.monitor_op(current, self.best):\n",
    "                self.best = current\n",
    "                self.wait = 0\n",
    "                self.RLRoP_patience = self.original_RLRoP_patience\n",
    "                # Save the model if the `save_best_model` flag is set\n",
    "                if self.save_best_model:\n",
    "                    model.save(self.save_path)\n",
    "                    if self.verbose > 0:\n",
    "                        print(f'\\nEpoch {self.current_epoch + 1}: Model saved to {self.save_path}')\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.ES_patience and self.early_stopping:\n",
    "                    self.stopped_epoch = self.current_epoch\n",
    "                    self.model.stop_training = True\n",
    "                    if self.verbose > 0:\n",
    "                       print(f'\\nEpoch {self.current_epoch + 1}: early stopping')\n",
    "                if self.wait >= self.RLRoP_patience and self.RLRoP:\n",
    "                    old_lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "                    new_lr = old_lr * self.factor\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "                    self.RLRoP_patience += self.original_RLRoP_patience\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Plot after training ends\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.batch_numbers[50:], self.batch_accuracies[50:], color='blue', label='Training Accuracy',\n",
    "                 linestyle='-')\n",
    "        plt.plot(self.val_indices, self.validation_accuracies, color='red', label='Validation Accuracy',\n",
    "                 linestyle='-')\n",
    "        for epoch in self.epoch_indices:\n",
    "            plt.axvline(x=(epoch + 1) * self.params['steps'], color='grey', linestyle='--', linewidth=1)\n",
    "            plt.text(\n",
    "                (epoch + 0.5) * self.params['steps'],\n",
    "                plt.ylim()[1] * 0.95,\n",
    "                f'Epoch {epoch}',\n",
    "                color='grey',\n",
    "                rotation=90,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right'\n",
    "            )\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        #plt.show()\n",
    "\n",
    "        if self.validation_losses:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(self.batch_numbers[50:], self.batch_losses[50:], color='blue', label='Training Loss',\n",
    "                     linestyle='-')\n",
    "            plt.plot(self.val_indices, self.validation_losses, linestyle='-', color='red', label='Validation Loss')\n",
    "            for epoch in self.epoch_indices:\n",
    "                plt.axvline(x=(epoch + 1) * self.params['steps'], color='grey', linestyle='--', linewidth=1)\n",
    "                plt.text(\n",
    "                    (epoch + 0.5) * self.params['steps'],\n",
    "                    plt.ylim()[1] * 0.85,\n",
    "                    f'Epoch {epoch}',\n",
    "                    color='grey',\n",
    "                    rotation=90,\n",
    "                    verticalalignment='top',\n",
    "                    horizontalalignment='right'\n",
    "                )\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "\n",
    "class CustomTunerCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, epochs, batch_size, validation_steps):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_steps = validation_steps\n",
    "\n",
    "    def on_epoch_end(self, epoch):\n",
    "        if epoch + 1 >= self.epochs:\n",
    "            self.model.stop_training = True\n"
   ],
   "id": "99a2f04a22e5a1a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if fine_tune == False:\n",
    "    # Regular training without hyperparameter tuning\n",
    "    model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
    "    \n",
    "    print('Starting model training...')\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=loss,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    num_validations = 3\n",
    "\n",
    "    plotcall = BatchAccCall(val_data=val_dataset, num_validations=num_validations, filepath=os.path.join(pthDL, 'best_model_net.keras'))\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # Warmup model (if needed)\n",
    "    model_warmup = models.Sequential([\n",
    "        layers.Dense(10, activation='relu', input_shape=(784,)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model_warmup.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    x_train = np.random.random((1000, 784))\n",
    "    y_train = np.random.randint(10, size=(1000,))\n",
    "    model_warmup.fit(x_train, y_train, epochs=1, batch_size=32, verbose=0)\n",
    "\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, callbacks=[plotcall], verbose=1, epochs=8)\n",
    "\n",
    "else:\n",
    "    # Hyperparameter tuning with Keras Tuner\n",
    "    def build_model(hp):\n",
    "        model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=20,\n",
    "        directory='keras_tuner_dir',\n",
    "        project_name='deeplabv3plus_tuning'\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    tuner.search(train_dataset, \n",
    "                 epochs=8, \n",
    "                 validation_data=val_dataset,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(patience=2)])\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    # You can retrain the best model on the full dataset if needed\n",
    "    # history = best_model.fit(train_dataset, validation_data=val_dataset, epochs=8)\n",
    "\n",
    "    model = best_model  # Use the best model for saving later\n",
    "\n",
    "training_time = time.time() - start\n",
    "hours, rem = divmod(training_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(f\"Training time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "\n",
    "# Save model\n",
    "print('Saving model...')\n",
    "model.save(os.path.join(pthDL, 'net.keras'))\n",
    "\n",
    "# Save history\n",
    "if fine_tune:\n",
    "    data['history'] = tuner.get_best_models()[0].history.history\n",
    "else:\n",
    "    data['history'] = history.history\n",
    "\n",
    "with open(os.path.join(pthDL, 'net.pkl'), 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ],
   "id": "2f672e187fb3087"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
