{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-31T21:24:01.002492Z",
     "start_time": "2024-05-31T21:23:59.314493Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Author: Valentina Matos (Johns Hopkins - Wirtz/Kiemen Lab)\n",
    "Date: May 22, 2024\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import gc\n",
    "from GPUtil import showUtilization as gpu_usage"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:27.765728Z",
     "start_time": "2024-05-29T15:47:27.750727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Inputs\n",
    "pthDL = r'\\\\10.99.68.52\\Kiemendata\\Valentina Matos\\coda to python\\test model\\model test tiles'\n"
   ],
   "id": "bdde62bd51e6a65f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:28.486079Z",
     "start_time": "2024-05-29T15:47:27.768727Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Load variables from pickle file\n",
    "with open(os.path.join(pthDL, 'net.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    sxy, classNames, nm = data['sxy'], data['classNames'], data['nm']\n",
    "\n",
    "if 'net' in data:\n",
    "    raise ValueError(f\"A network has already been trained for model {nm}. Choose a new model name to retrain.\")\n",
    "else:\n",
    "    # rest of the code here\n",
    "    pass  # --delete pass when the rest of the code is placed below the else statement\n",
    "\n"
   ],
   "id": "a1c3c1f9e1ca7572",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:28.502080Z",
     "start_time": "2024-05-29T15:47:28.487080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths to training and validation datasets:\n",
    "classes = list(range(1, len(classNames)))\n",
    "nmim = 'im'\n",
    "nmlabel = 'label'\n",
    "\n",
    "pthTrain = os.path.join(pthDL, 'training')\n",
    "pthVal = os.path.join(pthDL, 'validation')\n",
    "\n",
    "Train_HE = os.path.join(pthTrain, nmim)\n",
    "Train_label = os.path.join(pthTrain, nmlabel)\n",
    "\n",
    "Validation_HE = os.path.join(pthVal, nmim)\n",
    "Validation_label = os.path.join(pthVal, nmlabel)"
   ],
   "id": "6402a3986136d276",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:28.518080Z",
     "start_time": "2024-05-29T15:47:28.503081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model input data blueprint\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, label_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        self.images = sorted(os.listdir(image_dir))  # Sort the image paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.images[idx])\n",
    "        label_name = os.path.join(self.label_dir, self.images[idx])\n",
    "        \n",
    "        # Check if both image and label files exist\n",
    "        if not os.path.exists(img_name) or not os.path.exists(label_name):\n",
    "            raise FileNotFoundError(f\"Image or label file not found: {img_name}, {label_name}\")\n",
    "\n",
    "\n",
    "        image = Image.open(img_name).convert(\"RGB\")  # Convert image to RGB\n",
    "        label = Image.open(label_name).convert(\"L\")  # Convert label to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.label_transform:\n",
    "            label = self.label_transform(label)\n",
    "            label = label.squeeze(0).long()  # Remove the channel dimension and convert to LongTensor\n",
    "\n",
    "        return image, label"
   ],
   "id": "c10f19ec8a228060",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:28.534080Z",
     "start_time": "2024-05-29T15:47:28.519080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define data transformations\n",
    "data_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  # Resize to the model's expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # normalization according to imageNet stats\n",
    "])"
   ],
   "id": "6b3c6bc6ffae0655",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:28.550080Z",
     "start_time": "2024-05-29T15:47:28.535080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ],
   "id": "31c98043a10d46ee",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:29.559437Z",
     "start_time": "2024-05-29T15:47:28.551080Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Make datasets\n",
    "train_dataset = CustomDataset(image_dir=Train_HE, label_dir=Train_label, transform=data_transform,\n",
    "                              label_transform=label_transform)\n",
    "val_dataset = CustomDataset(image_dir=Validation_HE, label_dir=Validation_label, transform=data_transform,\n",
    "                            label_transform=label_transform)\n"
   ],
   "id": "fce465bc852f1b4b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:29.574999Z",
     "start_time": "2024-05-29T15:47:29.560473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create data loaders\n",
    "batch_size = 4  # datapoints per 'mini-batch' - ideally a small power of 2 (32, 64, 128, or 256)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(type(train_loader))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False)\n"
   ],
   "id": "9377a975faddb2de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:29.590438Z",
     "start_time": "2024-05-29T15:47:29.576437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Fucntion to free up GPU\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()\n",
    "\n",
    "    # Check allocated memory before clearing cache\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated()} bytes\")\n",
    "    print(f\"Max Memory Allocated: {torch.cuda.max_memory_allocated()} bytes\")\n",
    "\n",
    "    # Delete all tensors from GPU memory\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                del obj\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    gc.collect()  # Run garbage collection\n",
    "    torch.cuda.empty_cache()  # Empty the CUDA cache\n",
    "    torch.cuda.ipc_collect()  # Collect any unused memory\n",
    "\n",
    "    # Reset max memory allocated\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "    # Check allocated memory after clearing cache\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated()} bytes\")\n",
    "    print(f\"Max Memory Allocated: {torch.cuda.max_memory_allocated()} bytes\")\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ],
   "id": "d1472462976326ac",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:30.050670Z",
     "start_time": "2024-05-29T15:47:29.592128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# _______________________Model Initialization________________________#\n",
    "\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True) # old\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', weights='DEFAULT')\n",
    "\n",
    "# Learning rate - controls how much the network updates its weights in each minibatch\n",
    "initial_lr = 0.0005\n",
    "\n",
    "# Optimizer - algorithm that updates the model's weights based on the loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# Loss function - measures the difference between predictions and actual labels\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ],
   "id": "c752fb4db8c3a97",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Valentina/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:30.541196Z",
     "start_time": "2024-05-29T15:47:30.051670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ___________________________Model Training______________________________#\n",
    "\n",
    "# Free GPU Cache\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU device: {torch.cuda.get_device_name()}')\n",
    "    free_gpu_cache()\n"
   ],
   "id": "e5f599c2aaf3b909",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: NVIDIA GeForce RTX 3090\n",
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 23% |  3% |\n",
      "Memory Allocated: 0 bytes\n",
      "Max Memory Allocated: 0 bytes\n",
      "Memory Allocated: 0 bytes\n",
      "Max Memory Allocated: 0 bytes\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 16% |  3% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valentina\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\cuda\\memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:30.557202Z",
     "start_time": "2024-05-29T15:47:30.542197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Training Hyperparameters\n",
    "\n",
    "num_epochs = 8  # Number of epochs - maximum number of times to iterate through the entire dataset\n",
    "\n",
    "validation_patience = 7  # Validation patience - number of epochs with no improvement to stop training early\n",
    "\n",
    "lr_drop_factor = 0.75  # Learning rate drop factor - how much to reduce learning rate after each drop period\n",
    "\n",
    "lr_drop_period = 1  # Learning rate drop factor - how much to reduce learning rate after each drop period\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_drop_period, gamma=lr_drop_factor)\n"
   ],
   "id": "1ec035a400a81bf",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:47:30.573201Z",
     "start_time": "2024-05-29T15:47:30.558823Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "train_accuracies = []"
   ],
   "id": "36dd3d7838af81a7",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:03:19.477686Z",
     "start_time": "2024-05-29T15:47:30.574202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## START TRAINING:\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(type(device))\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Let's train bby!\")\n",
    "import time\n",
    "start_training_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Record the start time of the epoch\n",
    "    start_time_epoch = time.time()\n",
    "    print(f'\\n__________________Epoch {epoch+1}/{num_epochs}__________________')\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # move the input and model to GPU for speed if available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)['out']\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print training performance after each step\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate and print the time taken for the epoch\n",
    "    epoch_time = time.time() - start_time_epoch\n",
    "    hours_epoch, rem_epoch = divmod(epoch_time, 3600)\n",
    "    minutes_epoch, seconds_epoch = divmod(rem_epoch, 60)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] completed in {hours_epoch}h {minutes_epoch}m {seconds_epoch}s.\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = running_correct / total_samples\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)['out']\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f} -- Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    scheduler.step()\n",
    "    print(f\"Learning rate adjusted to: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "end_training_time = time.time() - start_training_time\n",
    "hours, rem = divmod(end_training_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "print('Finished Training')\n",
    "print(f\"Training time: {hours}h {minutes}m {seconds}s\")    "
   ],
   "id": "7411d026246a0d1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's train bby!\n",
      "\n",
      "__________________Epoch 1/8__________________\n",
      "Epoch [1/8], Step [1/375], Loss: 0.3486\n",
      "Epoch [1/8], Step [2/375], Loss: 0.2062\n",
      "Epoch [1/8], Step [3/375], Loss: 0.1106\n",
      "Epoch [1/8], Step [4/375], Loss: 0.0598\n",
      "Epoch [1/8], Step [5/375], Loss: 0.0254\n",
      "Epoch [1/8], Step [6/375], Loss: 0.0152\n",
      "Epoch [1/8], Step [7/375], Loss: 0.0167\n",
      "Epoch [1/8], Step [8/375], Loss: 0.0106\n",
      "Epoch [1/8], Step [9/375], Loss: 0.0103\n",
      "Epoch [1/8], Step [10/375], Loss: 0.0076\n",
      "Epoch [1/8], Step [11/375], Loss: 0.0083\n",
      "Epoch [1/8], Step [12/375], Loss: 0.0070\n",
      "Epoch [1/8], Step [13/375], Loss: 0.0061\n",
      "Epoch [1/8], Step [14/375], Loss: 0.0051\n",
      "Epoch [1/8], Step [15/375], Loss: 0.0048\n",
      "Epoch [1/8], Step [16/375], Loss: 0.0047\n",
      "Epoch [1/8], Step [17/375], Loss: 0.0055\n",
      "Epoch [1/8], Step [18/375], Loss: 0.0044\n",
      "Epoch [1/8], Step [19/375], Loss: 0.0040\n",
      "Epoch [1/8], Step [20/375], Loss: 0.0046\n",
      "Epoch [1/8], Step [21/375], Loss: 0.0034\n",
      "Epoch [1/8], Step [22/375], Loss: 0.0031\n",
      "Epoch [1/8], Step [23/375], Loss: 0.0030\n",
      "Epoch [1/8], Step [24/375], Loss: 0.0031\n",
      "Epoch [1/8], Step [25/375], Loss: 0.0028\n",
      "Epoch [1/8], Step [26/375], Loss: 0.0026\n",
      "Epoch [1/8], Step [27/375], Loss: 0.0024\n",
      "Epoch [1/8], Step [28/375], Loss: 0.0021\n",
      "Epoch [1/8], Step [29/375], Loss: 0.0024\n",
      "Epoch [1/8], Step [30/375], Loss: 0.0021\n",
      "Epoch [1/8], Step [31/375], Loss: 0.0023\n",
      "Epoch [1/8], Step [32/375], Loss: 0.0020\n",
      "Epoch [1/8], Step [33/375], Loss: 0.0020\n",
      "Epoch [1/8], Step [34/375], Loss: 0.0018\n",
      "Epoch [1/8], Step [35/375], Loss: 0.0019\n",
      "Epoch [1/8], Step [36/375], Loss: 0.0018\n",
      "Epoch [1/8], Step [37/375], Loss: 0.0018\n",
      "Epoch [1/8], Step [38/375], Loss: 0.0019\n",
      "Epoch [1/8], Step [39/375], Loss: 0.0017\n",
      "Epoch [1/8], Step [40/375], Loss: 0.0017\n",
      "Epoch [1/8], Step [41/375], Loss: 0.0015\n",
      "Epoch [1/8], Step [42/375], Loss: 0.0015\n",
      "Epoch [1/8], Step [43/375], Loss: 0.0015\n",
      "Epoch [1/8], Step [44/375], Loss: 0.0015\n",
      "Epoch [1/8], Step [45/375], Loss: 0.0016\n",
      "Epoch [1/8], Step [46/375], Loss: 0.0014\n",
      "Epoch [1/8], Step [47/375], Loss: 0.0015\n",
      "Epoch [1/8], Step [48/375], Loss: 0.0014\n",
      "Epoch [1/8], Step [49/375], Loss: 0.0014\n",
      "Epoch [1/8], Step [50/375], Loss: 0.0013\n",
      "Epoch [1/8], Step [51/375], Loss: 0.0013\n",
      "Epoch [1/8], Step [52/375], Loss: 0.0013\n",
      "Epoch [1/8], Step [53/375], Loss: 0.0013\n",
      "Epoch [1/8], Step [54/375], Loss: 0.0012\n",
      "Epoch [1/8], Step [55/375], Loss: 0.0013\n",
      "Epoch [1/8], Step [56/375], Loss: 0.0013\n",
      "Epoch [1/8], Step [57/375], Loss: 0.0012\n",
      "Epoch [1/8], Step [58/375], Loss: 0.0012\n",
      "Epoch [1/8], Step [59/375], Loss: 0.0012\n",
      "Epoch [1/8], Step [60/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [61/375], Loss: 0.0012\n",
      "Epoch [1/8], Step [62/375], Loss: 0.0012\n",
      "Epoch [1/8], Step [63/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [64/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [65/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [66/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [67/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [68/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [69/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [70/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [71/375], Loss: 0.0011\n",
      "Epoch [1/8], Step [72/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [73/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [74/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [75/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [76/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [77/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [78/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [79/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [80/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [81/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [82/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [83/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [84/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [85/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [86/375], Loss: 0.0010\n",
      "Epoch [1/8], Step [87/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [88/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [89/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [90/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [91/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [92/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [93/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [94/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [95/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [96/375], Loss: 0.0009\n",
      "Epoch [1/8], Step [97/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [98/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [99/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [100/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [101/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [102/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [103/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [104/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [105/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [106/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [107/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [108/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [109/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [110/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [111/375], Loss: 0.0008\n",
      "Epoch [1/8], Step [112/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [113/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [114/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [115/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [116/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [117/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [118/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [119/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [120/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [121/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [122/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [123/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [124/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [125/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [126/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [127/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [128/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [129/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [130/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [131/375], Loss: 0.0007\n",
      "Epoch [1/8], Step [132/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [133/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [134/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [135/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [136/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [137/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [138/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [139/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [140/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [141/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [142/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [143/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [144/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [145/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [146/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [147/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [148/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [149/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [150/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [151/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [152/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [153/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [154/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [155/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [156/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [157/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [158/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [159/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [160/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [161/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [162/375], Loss: 0.0006\n",
      "Epoch [1/8], Step [163/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [164/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [165/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [166/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [167/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [168/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [169/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [170/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [171/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [172/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [173/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [174/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [175/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [176/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [177/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [178/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [179/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [180/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [181/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [182/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [183/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [184/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [185/375], Loss: 0.0005\n",
      "Epoch [1/8], Step [186/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [187/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [188/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [189/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [190/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [191/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [192/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [193/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [194/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [195/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [196/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [197/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [198/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [199/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [200/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [201/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [202/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [203/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [204/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [205/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [206/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [207/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [208/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [209/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [210/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [211/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [212/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [213/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [214/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [215/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [216/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [217/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [218/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [219/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [220/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [221/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [222/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [223/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [224/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [225/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [226/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [227/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [228/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [229/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [230/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [231/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [232/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [233/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [234/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [235/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [236/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [237/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [238/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [239/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [240/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [241/375], Loss: 0.0004\n",
      "Epoch [1/8], Step [242/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [243/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [244/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [245/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [246/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [247/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [248/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [249/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [250/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [251/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [252/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [253/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [254/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [255/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [256/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [257/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [258/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [259/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [260/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [261/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [262/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [263/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [264/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [265/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [266/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [267/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [268/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [269/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [270/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [271/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [272/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [273/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [274/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [275/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [276/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [277/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [278/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [279/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [280/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [281/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [282/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [283/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [284/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [285/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [286/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [287/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [288/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [289/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [290/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [291/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [292/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [293/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [294/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [295/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [296/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [297/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [298/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [299/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [300/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [301/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [302/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [303/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [304/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [305/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [306/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [307/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [308/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [309/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [310/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [311/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [312/375], Loss: 0.0003\n",
      "Epoch [1/8], Step [313/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [314/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [315/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [316/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [317/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [318/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [319/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [320/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [321/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [322/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [323/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [324/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [325/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [326/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [327/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [328/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [329/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [330/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [331/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [332/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [333/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [334/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [335/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [336/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [337/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [338/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [339/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [340/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [341/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [342/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [343/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [344/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [345/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [346/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [347/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [348/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [349/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [350/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [351/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [352/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [353/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [354/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [355/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [356/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [357/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [358/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [359/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [360/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [361/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [362/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [363/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [364/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [365/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [366/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [367/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [368/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [369/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [370/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [371/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [372/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [373/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [374/375], Loss: 0.0002\n",
      "Epoch [1/8], Step [375/375], Loss: 0.0002\n",
      "Epoch [1/8] completed in 0.0h 8.0m 22.196419715881348s.\n",
      "Epoch [1/8], Loss: 0.0029, Accuracy: 999348.5147 -- Validation Loss: 0.0002, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 0.000375\n",
      "\n",
      "__________________Epoch 2/8__________________\n",
      "Epoch [2/8], Step [1/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [2/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [3/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [4/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [5/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [6/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [7/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [8/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [9/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [10/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [11/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [12/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [13/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [14/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [15/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [16/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [17/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [18/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [19/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [20/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [21/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [22/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [23/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [24/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [25/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [26/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [27/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [28/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [29/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [30/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [31/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [32/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [33/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [34/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [35/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [36/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [37/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [38/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [39/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [40/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [41/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [42/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [43/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [44/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [45/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [46/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [47/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [48/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [49/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [50/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [51/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [52/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [53/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [54/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [55/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [56/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [57/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [58/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [59/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [60/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [61/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [62/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [63/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [64/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [65/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [66/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [67/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [68/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [69/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [70/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [71/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [72/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [73/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [74/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [75/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [76/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [77/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [78/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [79/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [80/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [81/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [82/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [83/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [84/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [85/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [86/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [87/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [88/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [89/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [90/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [91/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [92/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [93/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [94/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [95/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [96/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [97/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [98/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [99/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [100/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [101/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [102/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [103/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [104/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [105/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [106/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [107/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [108/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [109/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [110/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [111/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [112/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [113/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [114/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [115/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [116/375], Loss: 0.0002\n",
      "Epoch [2/8], Step [117/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [118/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [119/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [120/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [121/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [122/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [123/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [124/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [125/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [126/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [127/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [128/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [129/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [130/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [131/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [132/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [133/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [134/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [135/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [136/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [137/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [138/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [139/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [140/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [141/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [142/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [143/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [144/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [145/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [146/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [147/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [148/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [149/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [150/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [151/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [152/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [153/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [154/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [155/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [156/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [157/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [158/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [159/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [160/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [161/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [162/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [163/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [164/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [165/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [166/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [167/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [168/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [169/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [170/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [171/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [172/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [173/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [174/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [175/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [176/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [177/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [178/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [179/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [180/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [181/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [182/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [183/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [184/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [185/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [186/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [187/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [188/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [189/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [190/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [191/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [192/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [193/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [194/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [195/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [196/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [197/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [198/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [199/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [200/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [201/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [202/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [203/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [204/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [205/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [206/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [207/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [208/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [209/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [210/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [211/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [212/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [213/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [214/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [215/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [216/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [217/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [218/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [219/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [220/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [221/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [222/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [223/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [224/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [225/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [226/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [227/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [228/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [229/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [230/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [231/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [232/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [233/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [234/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [235/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [236/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [237/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [238/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [239/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [240/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [241/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [242/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [243/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [244/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [245/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [246/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [247/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [248/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [249/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [250/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [251/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [252/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [253/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [254/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [255/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [256/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [257/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [258/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [259/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [260/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [261/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [262/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [263/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [264/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [265/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [266/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [267/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [268/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [269/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [270/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [271/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [272/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [273/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [274/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [275/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [276/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [277/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [278/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [279/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [280/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [281/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [282/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [283/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [284/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [285/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [286/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [287/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [288/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [289/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [290/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [291/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [292/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [293/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [294/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [295/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [296/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [297/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [298/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [299/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [300/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [301/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [302/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [303/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [304/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [305/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [306/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [307/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [308/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [309/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [310/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [311/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [312/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [313/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [314/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [315/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [316/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [317/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [318/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [319/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [320/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [321/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [322/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [323/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [324/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [325/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [326/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [327/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [328/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [329/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [330/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [331/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [332/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [333/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [334/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [335/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [336/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [337/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [338/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [339/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [340/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [341/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [342/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [343/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [344/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [345/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [346/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [347/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [348/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [349/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [350/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [351/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [352/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [353/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [354/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [355/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [356/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [357/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [358/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [359/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [360/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [361/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [362/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [363/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [364/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [365/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [366/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [367/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [368/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [369/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [370/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [371/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [372/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [373/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [374/375], Loss: 0.0001\n",
      "Epoch [2/8], Step [375/375], Loss: 0.0001\n",
      "Epoch [2/8] completed in 0.0h 8.0m 13.610951662063599s.\n",
      "Epoch [2/8], Loss: 0.0001, Accuracy: 1000000.0000 -- Validation Loss: 0.0001, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 0.00028125000000000003\n",
      "\n",
      "__________________Epoch 3/8__________________\n",
      "Epoch [3/8], Step [1/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [2/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [3/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [4/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [5/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [6/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [7/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [8/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [9/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [10/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [11/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [12/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [13/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [14/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [15/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [16/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [17/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [18/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [19/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [20/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [21/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [22/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [23/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [24/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [25/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [26/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [27/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [28/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [29/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [30/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [31/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [32/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [33/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [34/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [35/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [36/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [37/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [38/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [39/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [40/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [41/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [42/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [43/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [44/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [45/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [46/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [47/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [48/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [49/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [50/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [51/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [52/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [53/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [54/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [55/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [56/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [57/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [58/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [59/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [60/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [61/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [62/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [63/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [64/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [65/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [66/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [67/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [68/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [69/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [70/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [71/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [72/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [73/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [74/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [75/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [76/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [77/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [78/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [79/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [80/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [81/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [82/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [83/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [84/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [85/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [86/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [87/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [88/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [89/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [90/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [91/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [92/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [93/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [94/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [95/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [96/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [97/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [98/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [99/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [100/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [101/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [102/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [103/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [104/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [105/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [106/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [107/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [108/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [109/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [110/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [111/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [112/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [113/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [114/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [115/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [116/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [117/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [118/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [119/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [120/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [121/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [122/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [123/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [124/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [125/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [126/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [127/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [128/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [129/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [130/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [131/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [132/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [133/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [134/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [135/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [136/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [137/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [138/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [139/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [140/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [141/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [142/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [143/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [144/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [145/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [146/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [147/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [148/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [149/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [150/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [151/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [152/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [153/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [154/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [155/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [156/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [157/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [158/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [159/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [160/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [161/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [162/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [163/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [164/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [165/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [166/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [167/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [168/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [169/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [170/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [171/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [172/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [173/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [174/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [175/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [176/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [177/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [178/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [179/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [180/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [181/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [182/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [183/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [184/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [185/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [186/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [187/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [188/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [189/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [190/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [191/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [192/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [193/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [194/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [195/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [196/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [197/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [198/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [199/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [200/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [201/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [202/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [203/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [204/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [205/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [206/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [207/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [208/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [209/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [210/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [211/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [212/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [213/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [214/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [215/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [216/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [217/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [218/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [219/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [220/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [221/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [222/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [223/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [224/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [225/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [226/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [227/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [228/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [229/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [230/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [231/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [232/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [233/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [234/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [235/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [236/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [237/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [238/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [239/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [240/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [241/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [242/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [243/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [244/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [245/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [246/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [247/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [248/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [249/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [250/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [251/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [252/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [253/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [254/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [255/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [256/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [257/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [258/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [259/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [260/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [261/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [262/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [263/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [264/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [265/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [266/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [267/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [268/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [269/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [270/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [271/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [272/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [273/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [274/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [275/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [276/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [277/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [278/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [279/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [280/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [281/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [282/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [283/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [284/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [285/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [286/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [287/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [288/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [289/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [290/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [291/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [292/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [293/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [294/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [295/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [296/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [297/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [298/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [299/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [300/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [301/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [302/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [303/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [304/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [305/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [306/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [307/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [308/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [309/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [310/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [311/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [312/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [313/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [314/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [315/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [316/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [317/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [318/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [319/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [320/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [321/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [322/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [323/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [324/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [325/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [326/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [327/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [328/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [329/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [330/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [331/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [332/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [333/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [334/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [335/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [336/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [337/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [338/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [339/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [340/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [341/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [342/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [343/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [344/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [345/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [346/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [347/375], Loss: 0.0000\n",
      "Epoch [3/8], Step [348/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [349/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [350/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [351/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [352/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [353/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [354/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [355/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [356/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [357/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [358/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [359/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [360/375], Loss: 0.0000\n",
      "Epoch [3/8], Step [361/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [362/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [363/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [364/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [365/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [366/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [367/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [368/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [369/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [370/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [371/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [372/375], Loss: 0.0001\n",
      "Epoch [3/8], Step [373/375], Loss: 0.0000\n",
      "Epoch [3/8], Step [374/375], Loss: 0.0000\n",
      "Epoch [3/8], Step [375/375], Loss: 0.0001\n",
      "Epoch [3/8] completed in 0.0h 8.0m 15.17748475074768s.\n",
      "Epoch [3/8], Loss: 0.0001, Accuracy: 1000000.0000 -- Validation Loss: 0.0001, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 0.00021093750000000002\n",
      "\n",
      "__________________Epoch 4/8__________________\n",
      "Epoch [4/8], Step [1/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [2/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [3/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [4/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [5/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [6/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [7/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [8/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [9/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [10/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [11/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [12/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [13/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [14/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [15/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [16/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [17/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [18/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [19/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [20/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [21/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [22/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [23/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [24/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [25/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [26/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [27/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [28/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [29/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [30/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [31/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [32/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [33/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [34/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [35/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [36/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [37/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [38/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [39/375], Loss: 0.0001\n",
      "Epoch [4/8], Step [40/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [41/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [42/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [43/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [44/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [45/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [46/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [47/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [48/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [49/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [50/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [51/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [52/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [53/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [54/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [55/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [56/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [57/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [58/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [59/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [60/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [61/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [62/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [63/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [64/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [65/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [66/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [67/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [68/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [69/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [70/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [71/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [72/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [73/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [74/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [75/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [76/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [77/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [78/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [79/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [80/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [81/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [82/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [83/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [84/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [85/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [86/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [87/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [88/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [89/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [90/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [91/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [92/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [93/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [94/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [95/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [96/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [97/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [98/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [99/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [100/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [101/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [102/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [103/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [104/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [105/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [106/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [107/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [108/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [109/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [110/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [111/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [112/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [113/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [114/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [115/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [116/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [117/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [118/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [119/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [120/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [121/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [122/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [123/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [124/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [125/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [126/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [127/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [128/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [129/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [130/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [131/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [132/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [133/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [134/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [135/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [136/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [137/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [138/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [139/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [140/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [141/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [142/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [143/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [144/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [145/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [146/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [147/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [148/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [149/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [150/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [151/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [152/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [153/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [154/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [155/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [156/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [157/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [158/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [159/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [160/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [161/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [162/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [163/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [164/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [165/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [166/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [167/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [168/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [169/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [170/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [171/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [172/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [173/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [174/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [175/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [176/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [177/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [178/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [179/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [180/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [181/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [182/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [183/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [184/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [185/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [186/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [187/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [188/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [189/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [190/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [191/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [192/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [193/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [194/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [195/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [196/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [197/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [198/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [199/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [200/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [201/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [202/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [203/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [204/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [205/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [206/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [207/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [208/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [209/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [210/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [211/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [212/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [213/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [214/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [215/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [216/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [217/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [218/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [219/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [220/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [221/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [222/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [223/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [224/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [225/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [226/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [227/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [228/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [229/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [230/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [231/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [232/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [233/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [234/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [235/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [236/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [237/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [238/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [239/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [240/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [241/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [242/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [243/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [244/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [245/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [246/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [247/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [248/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [249/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [250/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [251/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [252/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [253/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [254/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [255/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [256/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [257/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [258/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [259/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [260/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [261/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [262/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [263/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [264/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [265/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [266/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [267/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [268/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [269/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [270/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [271/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [272/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [273/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [274/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [275/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [276/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [277/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [278/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [279/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [280/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [281/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [282/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [283/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [284/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [285/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [286/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [287/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [288/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [289/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [290/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [291/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [292/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [293/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [294/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [295/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [296/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [297/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [298/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [299/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [300/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [301/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [302/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [303/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [304/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [305/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [306/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [307/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [308/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [309/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [310/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [311/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [312/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [313/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [314/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [315/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [316/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [317/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [318/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [319/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [320/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [321/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [322/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [323/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [324/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [325/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [326/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [327/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [328/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [329/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [330/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [331/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [332/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [333/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [334/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [335/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [336/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [337/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [338/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [339/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [340/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [341/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [342/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [343/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [344/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [345/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [346/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [347/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [348/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [349/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [350/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [351/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [352/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [353/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [354/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [355/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [356/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [357/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [358/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [359/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [360/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [361/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [362/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [363/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [364/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [365/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [366/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [367/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [368/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [369/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [370/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [371/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [372/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [373/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [374/375], Loss: 0.0000\n",
      "Epoch [4/8], Step [375/375], Loss: 0.0000\n",
      "Epoch [4/8] completed in 0.0h 8.0m 56.9531090259552s.\n",
      "Epoch [4/8], Loss: 0.0000, Accuracy: 1000000.0000 -- Validation Loss: 0.0000, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 0.00015820312500000003\n",
      "\n",
      "__________________Epoch 5/8__________________\n",
      "Epoch [5/8], Step [1/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [2/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [3/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [4/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [5/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [6/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [7/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [8/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [9/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [10/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [11/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [12/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [13/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [14/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [15/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [16/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [17/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [18/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [19/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [20/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [21/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [22/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [23/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [24/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [25/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [26/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [27/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [28/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [29/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [30/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [31/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [32/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [33/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [34/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [35/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [36/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [37/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [38/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [39/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [40/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [41/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [42/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [43/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [44/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [45/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [46/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [47/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [48/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [49/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [50/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [51/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [52/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [53/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [54/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [55/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [56/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [57/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [58/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [59/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [60/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [61/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [62/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [63/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [64/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [65/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [66/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [67/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [68/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [69/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [70/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [71/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [72/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [73/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [74/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [75/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [76/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [77/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [78/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [79/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [80/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [81/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [82/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [83/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [84/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [85/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [86/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [87/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [88/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [89/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [90/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [91/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [92/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [93/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [94/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [95/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [96/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [97/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [98/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [99/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [100/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [101/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [102/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [103/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [104/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [105/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [106/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [107/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [108/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [109/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [110/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [111/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [112/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [113/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [114/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [115/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [116/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [117/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [118/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [119/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [120/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [121/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [122/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [123/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [124/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [125/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [126/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [127/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [128/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [129/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [130/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [131/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [132/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [133/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [134/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [135/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [136/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [137/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [138/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [139/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [140/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [141/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [142/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [143/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [144/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [145/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [146/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [147/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [148/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [149/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [150/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [151/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [152/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [153/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [154/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [155/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [156/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [157/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [158/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [159/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [160/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [161/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [162/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [163/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [164/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [165/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [166/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [167/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [168/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [169/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [170/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [171/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [172/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [173/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [174/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [175/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [176/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [177/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [178/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [179/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [180/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [181/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [182/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [183/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [184/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [185/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [186/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [187/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [188/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [189/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [190/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [191/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [192/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [193/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [194/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [195/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [196/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [197/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [198/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [199/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [200/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [201/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [202/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [203/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [204/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [205/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [206/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [207/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [208/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [209/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [210/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [211/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [212/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [213/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [214/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [215/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [216/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [217/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [218/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [219/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [220/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [221/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [222/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [223/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [224/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [225/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [226/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [227/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [228/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [229/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [230/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [231/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [232/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [233/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [234/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [235/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [236/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [237/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [238/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [239/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [240/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [241/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [242/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [243/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [244/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [245/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [246/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [247/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [248/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [249/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [250/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [251/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [252/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [253/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [254/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [255/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [256/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [257/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [258/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [259/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [260/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [261/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [262/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [263/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [264/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [265/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [266/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [267/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [268/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [269/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [270/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [271/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [272/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [273/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [274/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [275/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [276/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [277/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [278/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [279/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [280/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [281/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [282/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [283/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [284/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [285/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [286/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [287/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [288/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [289/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [290/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [291/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [292/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [293/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [294/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [295/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [296/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [297/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [298/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [299/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [300/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [301/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [302/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [303/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [304/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [305/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [306/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [307/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [308/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [309/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [310/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [311/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [312/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [313/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [314/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [315/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [316/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [317/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [318/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [319/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [320/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [321/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [322/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [323/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [324/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [325/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [326/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [327/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [328/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [329/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [330/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [331/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [332/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [333/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [334/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [335/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [336/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [337/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [338/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [339/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [340/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [341/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [342/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [343/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [344/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [345/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [346/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [347/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [348/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [349/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [350/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [351/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [352/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [353/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [354/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [355/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [356/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [357/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [358/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [359/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [360/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [361/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [362/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [363/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [364/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [365/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [366/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [367/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [368/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [369/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [370/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [371/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [372/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [373/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [374/375], Loss: 0.0000\n",
      "Epoch [5/8], Step [375/375], Loss: 0.0000\n",
      "Epoch [5/8] completed in 0.0h 10.0m 50.97618126869202s.\n",
      "Epoch [5/8], Loss: 0.0000, Accuracy: 1000000.0000 -- Validation Loss: 0.0000, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 0.00011865234375000002\n",
      "\n",
      "__________________Epoch 6/8__________________\n",
      "Epoch [6/8], Step [1/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [2/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [3/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [4/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [5/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [6/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [7/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [8/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [9/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [10/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [11/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [12/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [13/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [14/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [15/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [16/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [17/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [18/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [19/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [20/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [21/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [22/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [23/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [24/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [25/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [26/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [27/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [28/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [29/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [30/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [31/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [32/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [33/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [34/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [35/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [36/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [37/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [38/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [39/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [40/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [41/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [42/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [43/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [44/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [45/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [46/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [47/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [48/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [49/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [50/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [51/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [52/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [53/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [54/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [55/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [56/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [57/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [58/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [59/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [60/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [61/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [62/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [63/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [64/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [65/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [66/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [67/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [68/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [69/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [70/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [71/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [72/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [73/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [74/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [75/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [76/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [77/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [78/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [79/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [80/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [81/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [82/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [83/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [84/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [85/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [86/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [87/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [88/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [89/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [90/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [91/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [92/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [93/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [94/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [95/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [96/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [97/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [98/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [99/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [100/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [101/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [102/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [103/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [104/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [105/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [106/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [107/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [108/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [109/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [110/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [111/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [112/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [113/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [114/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [115/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [116/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [117/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [118/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [119/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [120/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [121/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [122/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [123/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [124/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [125/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [126/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [127/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [128/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [129/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [130/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [131/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [132/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [133/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [134/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [135/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [136/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [137/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [138/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [139/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [140/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [141/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [142/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [143/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [144/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [145/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [146/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [147/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [148/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [149/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [150/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [151/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [152/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [153/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [154/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [155/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [156/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [157/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [158/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [159/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [160/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [161/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [162/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [163/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [164/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [165/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [166/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [167/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [168/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [169/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [170/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [171/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [172/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [173/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [174/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [175/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [176/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [177/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [178/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [179/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [180/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [181/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [182/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [183/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [184/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [185/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [186/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [187/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [188/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [189/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [190/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [191/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [192/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [193/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [194/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [195/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [196/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [197/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [198/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [199/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [200/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [201/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [202/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [203/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [204/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [205/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [206/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [207/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [208/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [209/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [210/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [211/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [212/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [213/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [214/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [215/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [216/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [217/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [218/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [219/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [220/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [221/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [222/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [223/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [224/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [225/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [226/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [227/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [228/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [229/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [230/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [231/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [232/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [233/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [234/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [235/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [236/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [237/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [238/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [239/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [240/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [241/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [242/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [243/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [244/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [245/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [246/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [247/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [248/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [249/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [250/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [251/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [252/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [253/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [254/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [255/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [256/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [257/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [258/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [259/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [260/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [261/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [262/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [263/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [264/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [265/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [266/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [267/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [268/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [269/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [270/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [271/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [272/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [273/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [274/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [275/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [276/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [277/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [278/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [279/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [280/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [281/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [282/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [283/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [284/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [285/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [286/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [287/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [288/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [289/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [290/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [291/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [292/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [293/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [294/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [295/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [296/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [297/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [298/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [299/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [300/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [301/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [302/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [303/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [304/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [305/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [306/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [307/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [308/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [309/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [310/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [311/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [312/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [313/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [314/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [315/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [316/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [317/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [318/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [319/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [320/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [321/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [322/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [323/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [324/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [325/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [326/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [327/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [328/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [329/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [330/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [331/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [332/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [333/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [334/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [335/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [336/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [337/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [338/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [339/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [340/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [341/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [342/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [343/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [344/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [345/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [346/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [347/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [348/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [349/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [350/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [351/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [352/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [353/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [354/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [355/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [356/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [357/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [358/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [359/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [360/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [361/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [362/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [363/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [364/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [365/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [366/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [367/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [368/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [369/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [370/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [371/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [372/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [373/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [374/375], Loss: 0.0000\n",
      "Epoch [6/8], Step [375/375], Loss: 0.0000\n",
      "Epoch [6/8] completed in 0.0h 8.0m 14.464558839797974s.\n",
      "Epoch [6/8], Loss: 0.0000, Accuracy: 1000000.0000 -- Validation Loss: 0.0000, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 8.898925781250001e-05\n",
      "\n",
      "__________________Epoch 7/8__________________\n",
      "Epoch [7/8], Step [1/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [2/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [3/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [4/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [5/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [6/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [7/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [8/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [9/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [10/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [11/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [12/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [13/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [14/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [15/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [16/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [17/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [18/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [19/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [20/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [21/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [22/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [23/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [24/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [25/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [26/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [27/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [28/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [29/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [30/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [31/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [32/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [33/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [34/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [35/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [36/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [37/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [38/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [39/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [40/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [41/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [42/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [43/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [44/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [45/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [46/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [47/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [48/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [49/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [50/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [51/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [52/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [53/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [54/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [55/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [56/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [57/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [58/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [59/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [60/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [61/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [62/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [63/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [64/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [65/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [66/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [67/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [68/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [69/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [70/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [71/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [72/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [73/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [74/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [75/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [76/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [77/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [78/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [79/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [80/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [81/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [82/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [83/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [84/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [85/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [86/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [87/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [88/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [89/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [90/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [91/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [92/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [93/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [94/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [95/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [96/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [97/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [98/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [99/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [100/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [101/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [102/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [103/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [104/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [105/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [106/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [107/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [108/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [109/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [110/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [111/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [112/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [113/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [114/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [115/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [116/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [117/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [118/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [119/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [120/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [121/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [122/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [123/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [124/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [125/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [126/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [127/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [128/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [129/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [130/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [131/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [132/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [133/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [134/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [135/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [136/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [137/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [138/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [139/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [140/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [141/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [142/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [143/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [144/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [145/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [146/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [147/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [148/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [149/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [150/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [151/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [152/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [153/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [154/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [155/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [156/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [157/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [158/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [159/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [160/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [161/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [162/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [163/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [164/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [165/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [166/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [167/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [168/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [169/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [170/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [171/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [172/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [173/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [174/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [175/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [176/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [177/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [178/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [179/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [180/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [181/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [182/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [183/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [184/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [185/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [186/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [187/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [188/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [189/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [190/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [191/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [192/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [193/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [194/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [195/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [196/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [197/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [198/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [199/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [200/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [201/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [202/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [203/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [204/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [205/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [206/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [207/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [208/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [209/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [210/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [211/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [212/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [213/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [214/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [215/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [216/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [217/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [218/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [219/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [220/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [221/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [222/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [223/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [224/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [225/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [226/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [227/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [228/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [229/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [230/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [231/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [232/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [233/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [234/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [235/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [236/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [237/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [238/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [239/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [240/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [241/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [242/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [243/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [244/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [245/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [246/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [247/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [248/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [249/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [250/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [251/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [252/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [253/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [254/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [255/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [256/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [257/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [258/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [259/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [260/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [261/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [262/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [263/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [264/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [265/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [266/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [267/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [268/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [269/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [270/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [271/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [272/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [273/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [274/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [275/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [276/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [277/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [278/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [279/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [280/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [281/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [282/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [283/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [284/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [285/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [286/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [287/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [288/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [289/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [290/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [291/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [292/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [293/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [294/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [295/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [296/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [297/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [298/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [299/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [300/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [301/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [302/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [303/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [304/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [305/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [306/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [307/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [308/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [309/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [310/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [311/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [312/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [313/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [314/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [315/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [316/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [317/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [318/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [319/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [320/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [321/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [322/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [323/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [324/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [325/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [326/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [327/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [328/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [329/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [330/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [331/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [332/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [333/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [334/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [335/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [336/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [337/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [338/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [339/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [340/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [341/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [342/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [343/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [344/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [345/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [346/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [347/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [348/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [349/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [350/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [351/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [352/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [353/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [354/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [355/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [356/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [357/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [358/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [359/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [360/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [361/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [362/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [363/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [364/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [365/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [366/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [367/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [368/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [369/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [370/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [371/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [372/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [373/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [374/375], Loss: 0.0000\n",
      "Epoch [7/8], Step [375/375], Loss: 0.0000\n",
      "Epoch [7/8] completed in 0.0h 8.0m 16.02788805961609s.\n",
      "Epoch [7/8], Loss: 0.0000, Accuracy: 1000000.0000 -- Validation Loss: 0.0000, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 6.674194335937501e-05\n",
      "\n",
      "__________________Epoch 8/8__________________\n",
      "Epoch [8/8], Step [1/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [2/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [3/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [4/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [5/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [6/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [7/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [8/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [9/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [10/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [11/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [12/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [13/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [14/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [15/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [16/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [17/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [18/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [19/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [20/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [21/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [22/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [23/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [24/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [25/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [26/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [27/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [28/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [29/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [30/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [31/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [32/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [33/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [34/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [35/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [36/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [37/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [38/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [39/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [40/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [41/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [42/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [43/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [44/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [45/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [46/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [47/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [48/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [49/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [50/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [51/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [52/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [53/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [54/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [55/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [56/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [57/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [58/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [59/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [60/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [61/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [62/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [63/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [64/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [65/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [66/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [67/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [68/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [69/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [70/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [71/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [72/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [73/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [74/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [75/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [76/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [77/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [78/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [79/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [80/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [81/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [82/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [83/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [84/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [85/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [86/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [87/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [88/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [89/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [90/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [91/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [92/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [93/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [94/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [95/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [96/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [97/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [98/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [99/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [100/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [101/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [102/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [103/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [104/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [105/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [106/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [107/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [108/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [109/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [110/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [111/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [112/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [113/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [114/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [115/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [116/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [117/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [118/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [119/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [120/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [121/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [122/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [123/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [124/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [125/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [126/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [127/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [128/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [129/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [130/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [131/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [132/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [133/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [134/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [135/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [136/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [137/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [138/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [139/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [140/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [141/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [142/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [143/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [144/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [145/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [146/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [147/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [148/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [149/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [150/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [151/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [152/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [153/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [154/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [155/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [156/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [157/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [158/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [159/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [160/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [161/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [162/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [163/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [164/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [165/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [166/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [167/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [168/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [169/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [170/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [171/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [172/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [173/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [174/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [175/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [176/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [177/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [178/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [179/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [180/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [181/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [182/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [183/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [184/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [185/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [186/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [187/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [188/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [189/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [190/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [191/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [192/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [193/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [194/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [195/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [196/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [197/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [198/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [199/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [200/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [201/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [202/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [203/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [204/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [205/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [206/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [207/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [208/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [209/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [210/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [211/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [212/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [213/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [214/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [215/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [216/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [217/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [218/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [219/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [220/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [221/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [222/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [223/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [224/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [225/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [226/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [227/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [228/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [229/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [230/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [231/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [232/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [233/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [234/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [235/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [236/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [237/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [238/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [239/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [240/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [241/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [242/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [243/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [244/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [245/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [246/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [247/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [248/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [249/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [250/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [251/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [252/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [253/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [254/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [255/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [256/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [257/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [258/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [259/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [260/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [261/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [262/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [263/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [264/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [265/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [266/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [267/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [268/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [269/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [270/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [271/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [272/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [273/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [274/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [275/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [276/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [277/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [278/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [279/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [280/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [281/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [282/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [283/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [284/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [285/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [286/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [287/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [288/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [289/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [290/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [291/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [292/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [293/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [294/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [295/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [296/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [297/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [298/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [299/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [300/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [301/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [302/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [303/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [304/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [305/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [306/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [307/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [308/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [309/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [310/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [311/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [312/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [313/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [314/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [315/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [316/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [317/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [318/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [319/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [320/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [321/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [322/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [323/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [324/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [325/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [326/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [327/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [328/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [329/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [330/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [331/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [332/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [333/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [334/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [335/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [336/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [337/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [338/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [339/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [340/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [341/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [342/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [343/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [344/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [345/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [346/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [347/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [348/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [349/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [350/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [351/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [352/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [353/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [354/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [355/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [356/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [357/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [358/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [359/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [360/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [361/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [362/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [363/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [364/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [365/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [366/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [367/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [368/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [369/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [370/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [371/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [372/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [373/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [374/375], Loss: 0.0000\n",
      "Epoch [8/8], Step [375/375], Loss: 0.0000\n",
      "Epoch [8/8] completed in 0.0h 8.0m 17.94167470932007s.\n",
      "Epoch [8/8], Loss: 0.0000, Accuracy: 1000000.0000 -- Validation Loss: 0.0000, Validation Accuracy: 1000000.0000\n",
      "Learning rate adjusted to: 5.0056457519531256e-05\n",
      "Finished Training\n",
      "Training time: 1.0h 15.0m 48.724480628967285s\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:03:22.077659Z",
     "start_time": "2024-05-29T17:03:19.478686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save model\n",
    "print('Saving model weights...')\n",
    "data['net'] = model.state_dict()\n",
    "\n",
    "with open(os.path.join(pthDL, 'net.pkl'), 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ],
   "id": "1d54961035e72e29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model weights...\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:03:22.409650Z",
     "start_time": "2024-05-29T17:03:22.078659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "data = {\n",
    "    'epoch': list(range(1, num_epochs + 1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'train_accuracy': train_accuracies,\n",
    "    'val_accuracy': val_accuracies\n",
    "}\n",
    "history_df = pd.DataFrame(data)\n",
    "\n",
    "# Plot loss and accuracy in a single figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=history_df, x='epoch', y='train_loss', label='Training Loss')\n",
    "sns.lineplot(data=history_df, x='epoch', y='val_loss', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=history_df, x='epoch', y='train_accuracy', label='Training Accuracy')\n",
    "sns.lineplot(data=history_df, x='epoch', y='val_accuracy', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6436d9be9e771b19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:18:48.110584Z",
     "start_time": "2024-05-29T17:18:48.096584Z"
    }
   },
   "cell_type": "code",
   "source": "history_df",
   "id": "93e395b835de9017",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   epoch  train_loss  val_loss  train_accuracy  val_accuracy\n",
       "0      1    0.002893  0.000202   999348.514667     1000000.0\n",
       "1      2    0.000127  0.000087  1000000.000000     1000000.0\n",
       "2      3    0.000065  0.000053  1000000.000000     1000000.0\n",
       "3      4    0.000043  0.000038  1000000.000000     1000000.0\n",
       "4      5    0.000032  0.000030  1000000.000000     1000000.0\n",
       "5      6    0.000026  0.000026  1000000.000000     1000000.0\n",
       "6      7    0.000022  0.000022  1000000.000000     1000000.0\n",
       "7      8    0.000019  0.000019  1000000.000000     1000000.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>999348.514667</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:24:21.404890Z",
     "start_time": "2024-05-31T21:24:07.510921Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Load the model and weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', weights='DEFAULT')\n",
    "model.to(device)\n",
    "\n",
    "pthDL = r'\\\\10.99.68.52\\Kiemendata\\Valentina Matos\\coda to python\\test model\\model test tiles'\n",
    "with open(os.path.join(pthDL, 'net.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    model.load_state_dict(data['net'])"
   ],
   "id": "dd3c83ec209a3985",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Valentina/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:52:01.432355Z",
     "start_time": "2024-05-31T21:47:54.274078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import tifffile\n",
    "import numpy as np\n",
    "\n",
    "input_image_path = r'\\\\10.99.68.52\\Kiemendata\\Valentina Matos\\coda to python\\test model\\5x\\SG_013_0061.tif'\n",
    "# Define the data transformation\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the input image using tifffile\n",
    "input_image = tifffile.imread(input_image_path)\n",
    "\n",
    "# Convert the image to RGB format if it has more than 3 channels\n",
    "if input_image.ndim > 2 and input_image.shape[2] > 3:\n",
    "    input_image = input_image[:, :, :3]\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "input_tensor = torch.from_numpy(input_image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "# Apply the data transformation\n",
    "input_tensor = data_transform(input_tensor)\n",
    "input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform segmentation on the input image\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)['out']\n",
    "\n",
    "# Process the output (e.g., apply argmax to get the segmentation mask)\n",
    "segmentation_mask = output.argmax(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Prepare the segmentation mask for visualization\n",
    "segmentation_mask = np.uint8(segmentation_mask)\n",
    "segmentation_mask = np.repeat(segmentation_mask[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "# Convert the input tensor to a NumPy array for visualization\n",
    "input_image_numpy = input_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Prepare the segmented image by overlaying the segmentation mask on the input image\n",
    "segmented_image = input_image_numpy * 0.5 + segmentation_mask * 0.5"
   ],
   "id": "927da3db2195cc31",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.10 GiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 57.64 GiB is allocated by PyTorch, and 14.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 32\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Perform segmentation on the input image\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 32\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_batch\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Process the output (e.g., apply argmax to get the segmentation mask)\u001B[39;00m\n\u001B[0;32m     35\u001B[0m segmentation_mask \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torchvision\\models\\segmentation\\_utils.py:23\u001B[0m, in \u001B[0;36m_SimpleSegmentationModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     21\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m:]\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# contract: features is a dict of tensors\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m result \u001B[38;5;241m=\u001B[39m OrderedDict()\n\u001B[0;32m     26\u001B[0m x \u001B[38;5;241m=\u001B[39m features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torchvision\\models\\_utils.py:69\u001B[0m, in \u001B[0;36mIntermediateLayerGetter.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     67\u001B[0m out \u001B[38;5;241m=\u001B[39m OrderedDict()\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m---> 69\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_layers:\n\u001B[0;32m     71\u001B[0m         out_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_layers[name]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torchvision\\models\\resnet.py:158\u001B[0m, in \u001B[0;36mBottleneck.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    155\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn3(out)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 158\u001B[0m     identity \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownsample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m out \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m identity\n\u001B[0;32m    161\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(out)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    168\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[0;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CODA_python_afsjvm\\lib\\site-packages\\torch\\nn\\functional.py:2482\u001B[0m, in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m   2480\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m-> 2482\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2483\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[0;32m   2484\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 22.10 GiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 57.64 GiB is allocated by PyTorch, and 14.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # Display the input image, segmentation mask, and segmented image\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(input_image_numpy)\n",
    "axes[0].set_title('Input Image')\n",
    "axes[1].imshow(segmentation_mask)\n",
    "axes[1].set_title('Segmentation Mask')\n",
    "axes[2].imshow(segmented_image)\n",
    "axes[2].set_title('Segmented Image')\n",
    "plt.show()"
   ],
   "id": "6874b74243945d69"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
